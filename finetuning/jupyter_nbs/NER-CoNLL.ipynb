{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa4a2f9-85c9-41ca-8141-4dfe575ca60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "http_proxy_url = \"http://proxy.alcf.anl.gov:3128\"\n",
    "https_proxy_url = \"http://proxy.alcf.anl.gov:3128\"\n",
    "os.environ['http_proxy']=http_proxy_url\n",
    "os.environ['https_proxy']=https_proxy_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForTokenClassification, Trainer, TrainingArguments, AutoTokenizer, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbe6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "publishing_name=\"roberta-conll\"\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "def load_data():\n",
    "    dataset = load_dataset(\"eriktks/conll2003\")\n",
    "    return dataset\n",
    "\n",
    "def prepare_model_and_tokenizer(label_list):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"FacebookAI/roberta-base\",\n",
    "        num_labels=len(label_list),\n",
    "        id2label={i: label for i, label in enumerate(label_list)},\n",
    "        label2id={label: i for i, label in enumerate(label_list)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\", add_prefix_space=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Load metric\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    # Get the label list from your dataset or model config\n",
    "    label_list = model.config.id2label.values()\n",
    "\n",
    "    true_labels = [\n",
    "        [label_list[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load dataset\n",
    "    dataset = load_data()\n",
    "    \n",
    "    # Get label list\n",
    "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "    \n",
    "    # Prepare model and tokenizer\n",
    "    model, tokenizer = prepare_model_and_tokenizer(label_list)\n",
    "    \n",
    "    # Tokenize and align labels\n",
    "    tokenized_datasets = dataset.map(\n",
    "        lambda examples: tokenize_and_align_labels(examples, tokenizer),\n",
    "        batched=True,\n",
    "    )\n",
    "    \n",
    "    # Prepare data collator\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    # Prepare training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=publishing_name,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=4,\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    print(results)\n",
    "    \n",
    "    # Push the model to the Hugging Face Hub\n",
    "    trainer.push_to_hub(publishing_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env2",
   "language": "python",
   "name": "unsloth_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
