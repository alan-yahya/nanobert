{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset_builder, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import glob\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "os.environ['WANDB_API_KEY']= \"x\"\n",
    "os.environ[\"WANDB_PROJECT\"]=\"LLLM\"\n",
    "os.environ['WANDB_WATCH']=\"all\"\n",
    "os.environ['WANDB_LOG_MODEL']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a6977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With metrics per NER tag\n",
    "\n",
    "\n",
    "\n",
    "# Constants\n",
    "MAX_LENGTH = 128\n",
    "MODEL_NAME = \"m3rg-iitd/matscibert\"\n",
    "publishing_name= \"MatSciBERT800abstractsNER\"\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "FILE_PATH = r\"C:\\Users\\alan\\train.txt\"  # Update this with the actual path to your data file\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_ner_data(file_content):\n",
    "    sentences, labels = [], []\n",
    "    current_sentence, current_labels = [], []\n",
    "    \n",
    "    for line in file_content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                word, label = parts[0], parts[-1]\n",
    "                current_sentence.append(word)\n",
    "                current_labels.append(label)\n",
    "        elif current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "            current_sentence, current_labels = [], []\n",
    "    \n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        labels.append(current_labels)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=MAX_LENGTH)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Compute overall metrics\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Manually compute per-tag metrics\n",
    "    per_tag_metrics = {}\n",
    "    for tag in set(label_list):\n",
    "        # Filter predictions and references for the specific tag\n",
    "        tag_predictions = []\n",
    "        tag_labels = []\n",
    "        for pred, label in zip(true_predictions, true_labels):\n",
    "            filtered_pred = [1 if p == tag else 0 for p in pred]\n",
    "            filtered_label = [1 if l == tag else 0 for l in label]\n",
    "            tag_predictions.append(filtered_pred)\n",
    "            tag_labels.append(filtered_label)\n",
    "        \n",
    "        tag_results = metric.compute(predictions=tag_predictions, references=tag_labels)\n",
    "        per_tag_metrics[tag] = {\n",
    "            \"precision\": tag_results[\"overall_precision\"],\n",
    "            \"recall\": tag_results[\"overall_recall\"],\n",
    "            \"f1\": tag_results[\"overall_f1\"],\n",
    "            \"accuracy\": tag_results[\"overall_accuracy\"],\n",
    "        }\n",
    "\n",
    "    # Add per-tag metrics to the results\n",
    "    results[\"per_tag_metrics\"] = per_tag_metrics\n",
    "\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "        \"per_tag_metrics\": per_tag_metrics\n",
    "    }\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    global tokenizer, label_list, metric\n",
    "\n",
    "    # Load and preprocess data\n",
    "    file_content = read_file(FILE_PATH)\n",
    "    sentences, labels = read_ner_data(file_content)\n",
    "\n",
    "    # Create label mappings\n",
    "    label_list = sorted(list(set(label for sent_labels in labels for label in sent_labels)))\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    # Prepare dataset\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.2)\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\"tokens\": train_texts, \"ner_tags\": [[label2id[l] for l in label] for label in train_labels]})\n",
    "    val_dataset = Dataset.from_dict({\"tokens\": val_texts, \"ner_tags\": [[label2id[l] for l in label] for label in val_labels]})\n",
    "\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Tokenize and align labels\n",
    "    train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    # Evaluation metric\n",
    "    metric = load_metric(\"seqeval\")\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= publishing_name,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    # Print overall metrics\n",
    "    print(\"Overall Metrics:\")\n",
    "    print(f\"Precision: {results['eval_precision']:.4f}\")\n",
    "    print(f\"Recall: {results['eval_recall']:.4f}\")\n",
    "    print(f\"F1 Score: {results['eval_f1']:.4f}\")\n",
    "    print(f\"Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "    \n",
    "    # Print per-tag metrics\n",
    "    print(\"\\nPer-tag Metrics:\")\n",
    "    for tag, metrics in results['eval_per_tag_metrics'].items():\n",
    "        print(f\"\\n{tag}:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"  Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(\"./ner_model\")\n",
    "    trainer.push_to_hub()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
